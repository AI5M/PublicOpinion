{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import connDB\n",
    "import config as cfg\n",
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from os import system\n",
    "system(\"title PttCrawler\") #set cmd title\n",
    "\n",
    "conn = connDB.MyConnecter() #連接資料庫物件\n",
    "conn.connect() #開始連接\n",
    "\n",
    "PTT_URL = 'https://www.ptt.cc'\n",
    "hotboards = 'https://www.ptt.cc/bbs/hotboards.html'\n",
    "page_count = 50\n",
    "\n",
    "def getChildBoard(link):\n",
    "\tboard_count = 0\n",
    "\tresult = requests.get(url=link, cookies={'over18':'1'})\n",
    "\tsoup = BeautifulSoup(result.text,'html.parser')\n",
    "\tchild_list = soup.select('a.board')\n",
    "\tfor child in child_list:\n",
    "\t\t# try:\n",
    "\t\tchild_link = PTT_URL+child['href']\n",
    "\t\tif(child_link.split('/')[-3] == 'bbs'):\n",
    "\t\t\tboard_name = child.find(class_='board-name').text\n",
    "\t\t\tboard_class = child.find(class_='board-class').text\n",
    "\t\t\tboard_title = child.find(class_='board-title').text\n",
    "\t\t\tboard_count += 1\n",
    "\t\t\tparse_article(board_name, board_class, page_count)\n",
    "\t\t\t#print(child_link,board_name,board_class,board_title)\n",
    "\t\telse:\n",
    "\t\t\tboard_name = child.find(class_='board-name').text\n",
    "\t\t\tif(board_name == '0ClassRoot'):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# print('===========================','進入',child_link,'========================')\n",
    "\t\t\tboard_count += getChildBoard(child_link)\n",
    "\t\t# except:\n",
    "\t\t# \tpass\n",
    "\treturn board_count\n",
    "\n",
    "def parse_article(board_name, board_class,page_count):\n",
    "\tpage = 0\n",
    "\turl = PTT_URL + '/bbs/' + board_name + '/index.html'\n",
    "\twhile(page<page_count and url != PTT_URL + '/bbs/' + board_name + '/index1.html'):\n",
    "\t\tpage +=1\n",
    "\t\tresp = requests.get(url, cookies={'over18': '1'})\n",
    "\t\tif resp.status_code != 200:\n",
    "\t\t\tprint('invalid url:', resp.url)\n",
    "\t\t\treturn\n",
    "\t\tsoup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\t\tdivs = soup.find_all(\"div\", \"r-ent\")\n",
    "\t\tfor div in divs:\n",
    "\t\t\t# try:\n",
    "\t\t# ex. link would be <a href=\"/bbs/PublicServan/M.1127742013.A.240.html\">Re: [問題] 職等</a>\n",
    "\t\t\thref = div.find('a')['href']\n",
    "\t\t\tlink = PTT_URL + href\n",
    "\t\t\tarticle_id = re.sub('\\.html', '', href.split('/')[-1])\n",
    "\t\t\tprint(link,article_id)\n",
    "\t\t\tparse(link, article_id, board_name)\n",
    "\t\t\t# except:\n",
    "\t\t\t# \tpass\n",
    "        \n",
    "\t\tprev_page = soup.select('.btn.wide')[1]['href']\n",
    "\t\turl = PTT_URL + prev_page\n",
    "\t\t# print(url)\n",
    "\n",
    "def parse(link, article_id, board, timeout=3):\n",
    "\tresult = requests.get(url=link, cookies={'over18': '1'})\n",
    "\tif result.status_code != 200:\n",
    "\t\tprint('invalid url:', resp.url)\n",
    "\n",
    "\tsoup = BeautifulSoup(result.text, 'html.parser')\n",
    "\tmain_content = soup.find(id=\"main-content\")\n",
    "\tmetas = main_content.select('div.article-metaline') #上面欄位\n",
    "\tauthor = ''\n",
    "\ttitle = ''\n",
    "\tdate = ''\n",
    "\tif metas:\n",
    "\t\tauthor = metas[0].select('span.article-meta-value')[0].string if metas[0].select('span.article-meta-value')[0] else author\n",
    "\t\ttitle = metas[1].select('span.article-meta-value')[0].string if metas[1].select('span.article-meta-value')[0] else title\n",
    "\t\tdate = metas[2].select('span.article-meta-value')[0].string if metas[2].select('span.article-meta-value')[0] else date\n",
    "\n",
    "\t\t# remove meta nodes\n",
    "\t\tfor meta in metas:\n",
    "\t\t\tmeta.extract()\n",
    "\t\tfor meta in main_content.select('div.article-metaline-right'):\n",
    "\t\t\tmeta.extract()\n",
    "            \n",
    "\tpushes = main_content.find_all('div', class_='push')\n",
    "\tfor push in pushes:\n",
    "\t\tpush.extract()\n",
    "\n",
    "\ttry:\n",
    "\t\tip = main_content.find(text=re.compile(u'※ 發信站:'))\n",
    "\t\tip = re.search('[0-9]*\\.[0-9]*\\.[0-9]*\\.[0-9]*', ip).group()\n",
    "\texcept:\n",
    "\t\tip = \"None\"\n",
    "\n",
    "\tfiltered = [ v for v in main_content.stripped_strings if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--'] ]\n",
    "\texpr = re.compile(u(r'[^\\u4e00-\\u9fa5\\u3002\\uff1b\\uff0c\\uff1a\\u201c\\u201d\\uff08\\uff09\\u3001\\uff1f\\u300a\\u300b\\s\\w:/-_.?~%()]'))\n",
    "\tfor i in range(len(filtered)):\n",
    "\t\tfiltered[i] = re.sub(expr, '', filtered[i])\n",
    "\n",
    "\tfiltered = [_f for _f in filtered if _f]  # remove empty strings\n",
    "\tfiltered = [x for x in filtered if article_id not in x]  # remove last line containing the url of the article\n",
    "\tcontent = ' '.join(filtered)\n",
    "\tcontent = re.sub(r'(\\s)+', ' ', content)\n",
    "\n",
    "\t# push messages\n",
    "\tp, b, n = 0, 0, 0\n",
    "\tmessages = []\n",
    "\tfor push in pushes:\n",
    "\t\tif not push.find('span', 'push-tag'):\n",
    "\t\t\tcontinue\n",
    "\t\tpush_tag = push.find('span', 'push-tag').string.strip(' \\t\\n\\r')\n",
    "\t\tpush_userid = push.find('span', 'push-userid').string.strip(' \\t\\n\\r')\n",
    "\t\t# if find is None: find().strings -> list -> ' '.join; else the current way\n",
    "\t\tpush_content = push.find('span', 'push-content').strings\n",
    "\t\tpush_content = ' '.join(push_content)[1:].strip(' \\t\\n\\r')  # remove ':'\n",
    "\t\tpush_ipdatetime = push.find('span', 'push-ipdatetime').string.strip(' \\t\\n\\r')\n",
    "\t\tmessages.append( {'push_tag': push_tag, 'push_userid': push_userid, 'push_content': push_content, 'push_ipdatetime': push_ipdatetime} )\n",
    "\t\tif push_tag == u'推':\n",
    "\t\t\tp += 1\n",
    "\t\telif push_tag == u'噓':\n",
    "\t\t\tb += 1\n",
    "\t\telse:\n",
    "\t\t\tn += 1\n",
    "\n",
    "\t# count: 推噓文相抵後的數量; all: 推文總數\n",
    "\tmessage_count = {'all': p+b+n, 'count': p-b, 'push': p, 'boo': b, \"neutral\": n}\n",
    "\n",
    "\t# data = {'article_id' : connDB.escape_str(article_id), \n",
    "\t# \t\t'board_name' : connDB.escape_str(board), \n",
    "\t# \t\t'board_class' : connDB.escape_str(board_class),\n",
    "\t# \t\t'title' : connDB.escape_str(title),\n",
    "\t# \t\t'content' : connDB.escape_str(content),\n",
    "\t# \t\t'author_name' : connDB.escape_str(author),\n",
    "\t# \t\t'author_ip' : ip,\n",
    "\t# \t\t'create_time' : date,\n",
    "\t# \t\t'push' : p,\n",
    "\t# \t\t'shush' : b,\n",
    "\t# \t\t'neutral' : n,\n",
    "\t# \t\t'url' : link,}\n",
    "\n",
    "\t# conn.insert_replace(table='ptt', data=data) #replace to database table\n",
    "\n",
    "\t# print(link)\n",
    "\t# print(board)\n",
    "\t# print(board_class)\n",
    "\t# print(article_id)\n",
    "\t# print(title)\n",
    "\t# print(author)\n",
    "\t# print(date)\n",
    "\t# print(content)\n",
    "\t# print(ip)\n",
    "\t# print(p)\n",
    "\t# print(b)\n",
    "\t# print(n)\n",
    "\n",
    "\t# print('date',date)\n",
    "\t# print('msgs', messages)\n",
    "\t# print ('mscounts', message_count)\n",
    "\n",
    "getChildBoard(hotboards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
